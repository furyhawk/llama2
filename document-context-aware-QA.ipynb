{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88d7cc8c",
   "metadata": {},
   "source": [
    "# Text splitting by header\n",
    "\n",
    "Text splitting for vector storage often uses sentences or other delimiters [to keep related text together](https://www.pinecone.io/learn/chunking-strategies/). \n",
    "\n",
    "But many documents (such as `Markdown` files) have structure (headers) that can be explicitly used in splitting. \n",
    "\n",
    "The `MarkdownHeaderTextSplitter` lets a user split `Markdown` files files based on specified headers. \n",
    "\n",
    "This results in chunks that retain the header(s) that it came from in the metadata.\n",
    "\n",
    "This works nicely w/ `SelfQueryRetriever`.\n",
    "\n",
    "First, tell the retriever about our splits.\n",
    "\n",
    "Then, query based on the doc structure (e.g., \"summarize the doc introduction\"). \n",
    "\n",
    "Chunks only from that section of the Document will be filtered and used in chat / Q+A.\n",
    "\n",
    "Let's test this out on an [example Notion page](https://rlancemartin.notion.site/Auto-Evaluation-of-Metadata-Filtering-18502448c85240828f33716740f9574b?pvs=4)!\n",
    "\n",
    "First, I download the page to Markdown as explained [here](https://python.langchain.com/docs/ecosystem/integrations/notion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e587f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Notion page as a markdownfile file\n",
    "from langchain.document_loaders import NotionDirectoryLoader\n",
    "\n",
    "path = \"datasets/Notion_DB/\"\n",
    "loader = NotionDirectoryLoader(path)\n",
    "docs = loader.load()\n",
    "md_file = docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cd3fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create groups based on the section headers in our page\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"###\", \"Section\"),\n",
    "]\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "md_header_splits = markdown_splitter.split_text(md_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f73a609",
   "metadata": {},
   "source": [
    "Now, perform text splitting on the header grouped documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fbff95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our text splitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size = 500\n",
    "chunk_overlap = 0\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "all_splits = text_splitter.split_documents(md_header_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "418445be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# Auto-Evaluation of Metadata Filtering  \\n[Lance Martin](https://twitter.com/RLanceMartin)'),\n",
       " Document(page_content='Q+A systems often use a two-step approach: retrieve relevant text chunks and then synthesize them into an answer. There many ways to approach this. For example, we recently [discussed](https://blog.langchain.dev/auto-evaluation-of-anthropic-100k-context-window/) the Retriever-Less option (at bottom in the below diagram), highlighting the Anthropic 100k context window model. Metadata filtering is an alternative approach that pre-filters chunks based on a user-defined criteria in a VectorDB using', metadata={'Section': 'Introduction'}),\n",
       " Document(page_content='metadata tags prior to semantic search.', metadata={'Section': 'Introduction'}),\n",
       " Document(page_content='![Untitled](Auto-Evaluation%20of%20Metadata%20Filtering%201643790c6b344105abea1618b6d72d3d/Untitled.png)', metadata={'Section': 'Introduction'}),\n",
       " Document(page_content='I [previously built](https://twitter.com/RLanceMartin/status/1637852936238956546?s=20) a [QA app](https://lex-gpt.vercel.app/) based on the Lex Fridman podcast. This uses semantic search on Pinecone. However, it [failed](https://twitter.com/RLanceMartin/status/1639286900270964737?s=20) in cases where a user wanted to retrieve information about a specific episode (e.g., `summarize episode 53`) or in cases where a guest had been in multiple times and a user wanted information for a particular', metadata={'Section': 'Motivation'}),\n",
       " Document(page_content='episode (e.g., `what did Elon say in episode 252`).', metadata={'Section': 'Motivation'}),\n",
       " Document(page_content='In these cases, semantic search will look for the concept `episode 53` in the chunks, but instead we simply want to filter the chunks for `episode 53` and then perform semantic search to extract those that best summarize the episode. Metadata filtering does this, so long as we 1) we have a metadata filter for episode number and 2) we can extract the value from the query (e.g., `54` or `252`) that we want to extract. The LangChain `SelfQueryRetriever` does the latter (see', metadata={'Section': 'Motivation'}),\n",
       " Document(page_content='[docs](https://www.notion.so/Auto-Evaluation-of-Metadata-Filtering-1643790c6b344105abea1618b6d72d3d?pvs=21)), [splitting the user input](https://twitter.com/hwchase17/status/1656791490922967041?s=20) into a semantic query and a metadata filter (for [Pinecone](https://python.langchain.com/en/latest/modules/indexes/retrievers/examples/self_query.html) or [Chroma](https://python.langchain.com/en/latest/modules/indexes/retrievers/examples/chroma_self_query.html)).', metadata={'Section': 'Motivation'}),\n",
       " Document(page_content='We previously introduced [auto-evaluator](https://blog.langchain.dev/auto-evaluator-opportunities/), an open-source tool for grading LLM question-answer chains. Here, we extend auto-evaluator with a [lightweight Streamlit app](https://github.com/langchain-ai/auto-evaluator/tree/main/streamlit) that can connect to any existing Pinecone index. We add the ability to test metadata filtering using `SelfQueryRetriever` as well as some other approaches that we‚Äôve found to be useful, as discussed below.', metadata={'Section': 'Evaluation'}),\n",
       " Document(page_content='[ret_trim.mov](Auto-Evaluation%20of%20Metadata%20Filtering%201643790c6b344105abea1618b6d72d3d/ret_trim.mov)', metadata={'Section': 'Evaluation'}),\n",
       " Document(page_content='`SelfQueryRetriever` works well in [many cases](https://twitter.com/hwchase17/status/1656791488569954304/photo/1). For example, given [this test case](https://twitter.com/hwchase17/status/1656791488569954304?s=20):  \\n![Untitled](Auto-Evaluation%20of%20Metadata%20Filtering%201643790c6b344105abea1618b6d72d3d/Untitled%201.png)  \\nThe query can be nicely broken up into semantic query and metadata filter:  \\n```python\\nsemantic query: \"prompt injection\"', metadata={'Section': 'Testing'}),\n",
       " Document(page_content='metadata filter: \"webinar_name=agents in production\"\\n```  \\nBut, sometimes the metadata filter is not obvious based on the natural language in the question. For example, my [Lex-GPT](https://lex-gpt.vercel.app/) app used an episode ID tag derived from my initial scrape of the [Karpathy transcriptions](https://karpathy.ai/lexicap/index.html), e.g., I have `‚Äú0252‚Äù` for episode `252`. This means that the retriever will need to perform this translation step, as shown in the diagram below.', metadata={'Section': 'Testing'}),\n",
       " Document(page_content='![Untitled](Auto-Evaluation%20of%20Metadata%20Filtering%201643790c6b344105abea1618b6d72d3d/Untitled%202.png)', metadata={'Section': 'Testing'}),\n",
       " Document(page_content='`SelfQueryRetriever` will infer metadata filters from the query using `metadata_field_info`, which you can supply to the auto-evaluator [here](https://github.com/langchain-ai/auto-evaluator/blob/main/streamlit/self_query_retriever_lex.py). To address the above translation, we also test [kor](https://eyurtsev.github.io/kor/tutorial.html), a library for structured data extraction using an LLM. Using Kor, we can explicitly specify a transformation (e.g., `252` to `‚Äú0252‚Äù`) that the auto-evaluator', metadata={'Section': 'Testing'}),\n",
       " Document(page_content='will use in retrieval [here](https://github.com/langchain-ai/auto-evaluator/blob/main/streamlit/kor_retriever_lex.py).', metadata={'Section': 'Testing'}),\n",
       " Document(page_content='Below, we can see detailed results from the app:  \\n- Kor extraction is above to perform the transformation between query and metadata format ‚úÖ\\n- Self-querying attempts to filter using the episode ID (`252`) in the query and fails üö´\\n- Baseline returns docs from 3 different episodes (one from `252`), confusing the answer üö´', metadata={'Section': 'Testing'}),\n",
       " Document(page_content='Metadata filtering is a useful and powerful augmentation for semantic search. Extracting the metadata filter from the natural language query is a challenge. The `SelfQueryRetriever` does this well in many cases, but other LangChain integrations, such as Kor, can he very useful in cases where the filter definition is not obvious from the natural language query alone.', metadata={'Section': 'Conclusion'}),\n",
       " Document(page_content='Detailed results from the [Lex-GPT](https://lex-gpt.vercel.app/) app.  \\n|  | Question | Correct Answer | Generated Answer | Answer Grading | Retrieval Grading | Latency |\\n| --- | --- | --- | --- | --- | --- | --- |', metadata={'Section': 'Appendix'}),\n",
       " Document(page_content='| Pinecone | What does Elon Musk say about the self driving problem in episode 252? | Elon mentions that the self-driving problem is harder than he thought because you need to build a silicon equivalent of vision that maps from camera to vector space. But, he also mentions that the disengagements has been dropping rapidly and the probability of an accident on FSD will be less than that of the average human within a year. | In episode 252 of the Lex Fridman podcast, Elon Musk discusses the', metadata={'Section': 'Appendix'}),\n",
       " Document(page_content='self-driving problem and is impressed by what the Tesla Autopilot team is doing. They are using multitask learning to study the problem of driving and are formulating over a hundred tasks to perceive. They are deploying it and getting data back from people that run into trouble and are trying to figure out if they should add tasks or focus on each individual task separately. Musk acknowledges that there may be a long-term solution involving fancy self-supervised learning, but there is pressure', metadata={'Section': 'Appendix'}),\n",
       " Document(page_content='to take shortcuts due to the need to solve engineering problems in a relatively short time. | \"1. Relevance: The student\\'s answer does refer to Elon Musk discussing the self-driving problem in episode 252 of the Lex Fridman podcast. However, the details provided in the student\\'s answer do not match the true answer\\'s content.2. Conciseness: The student\\'s answer is not concise, as it provides a lot of information that is not directly related to the true answer.3. Correct: The student\\'s answer is', metadata={'Section': 'Appendix'}),\n",
       " Document(page_content='not correct, as it does not mention the key points from the true answer, such as the difficulty of building a silicon equivalent of vision and the rapid decrease in disengagements. Incorrect\" | \"GRADE: Correct JUSTIFICATION: All three documents contain information about Elon Musk, Tesla Autopilot, and self-driving technology. They also include discussions from the Lex Fridman Podcast, where the specific episode (252) is mentioned. These documents provide context and information that will help', metadata={'Section': 'Appendix'}),\n",
       " Document(page_content='the student arrive at the correct answer to the question.\" | 12.05227709 |', metadata={'Section': 'Appendix'}),\n",
       " Document(page_content=\"| Self-querying | What does Elon Musk say about the self driving problem in episode 252? | Elon mentions that the self-driving problem is harder than he thought because you need to build a silicon equivalent of vision that maps from camera to vector space. But, he also mentions that the disengagements has been dropping rapidly and the probability of an accident on FSD will be less than that of the average human within a year. | I'm sorry, I cannot provide an answer as there is no information\", metadata={'Section': 'Appendix'}),\n",
       " Document(page_content='about the specific episode mentioned. | \"1. The first criterion is relevance, which asks if the submission refers to a real quote from the text. The student answer does not provide any information about Elon Musk\\'s statement in episode 252, so it does not meet this criterion.2. The second criterion is conciseness, which asks if the answer is concise and to the point. The student answer is concise, but it does not provide any relevant information about the self-driving problem mentioned by Elon', metadata={'Section': 'Appendix'}),\n",
       " Document(page_content='Musk.3. The third criterion is correctness, which asks if the answer is correct. The student answer does not provide any information about Elon Musk\\'s statement, so it is not correct.Based on the reasoning above, the submission does not meet the criteria. Incorrect\" | \"GRADE: Incorrect JUSTIFICATION: There is only one document retrieved, and it does not contain any information related to the question. Therefore, the student cannot arrive at the correct answer using this document.\" | 7.095044136', metadata={'Section': 'Appendix'}),\n",
       " Document(page_content='|', metadata={'Section': 'Appendix'}),\n",
       " Document(page_content='| Kor Filtering | What does Elon Musk say about the self driving problem in episode 252? | Elon mentions that the self-driving problem is harder than he thought because you need to build a silicon equivalent of vision that maps from camera to vector space. But, he also mentions that the disengagements has been dropping rapidly and the probability of an accident on FSD will be less than that of the average human within a year. | Elon Musk says that the self-driving problem is harder than he', metadata={'Section': 'Appendix'}),\n",
       " Document(page_content='initially thought and that to solve it, they need to recreate what humans do to drive, which is to drive with optical sensors and biological neural nets. They need to recreate that in digital form, which means cameras with advanced neural nets in silicon form. The only way to solve for full self-driving is to do this. He also mentions that the rate of disengagements has been dropping rapidly, and it looks like the probability of an accident on FSD is less than that of the average human and', metadata={'Section': 'Appendix'}),\n",
       " Document(page_content='significantly less than before. It is looking quite likely that they will solve level four FSD next year. | \"1. The student answer refers to Elon Musk\\'s statement about the self-driving problem being harder than he initially thought, which is mentioned in the true answer.2. The student answer also discusses the need to recreate human driving using optical sensors and biological neural nets in digital form, which is related to building a silicon equivalent of vision mentioned in the true', metadata={'Section': 'Appendix'}),\n",
       " Document(page_content='answer.3. The student answer mentions the rate of disengagements dropping rapidly and the probability of an accident on FSD being less than that of the average human, which is also mentioned in the true answer.4. The student answer adds information about solving level four FSD next year, which is not mentioned in the true answer but is still relevant to the self-driving problem.5. The student answer is not as concise as the true answer, but it covers the main points and provides additional', metadata={'Section': 'Appendix'}),\n",
       " Document(page_content='relevant information. Correct\" | \"GRADE: Correct JUSTIFICATION: All three documents contain information about Elon Musk\\'s thoughts on self-driving technology and its progress. Specifically, Doc 2 provides a direct answer to the question, mentioning the difficulty of the problem and the improvements in disengagements.\" | 14.81295681 |', metadata={'Section': 'Appendix'})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd72546",
   "metadata": {},
   "source": [
    "This sets us up well do perform metadata filtering based on the document structure.\n",
    "\n",
    "Let's bring this all together by building a vectorstore first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b050b4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05da44ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../../models/zephyr-7b-beta.Q8_0.gguf (version unknown)\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q8_0     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:            blk.0.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:            blk.0.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:              blk.0.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:         blk.0.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:              blk.0.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.1.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:            blk.1.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:              blk.1.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:         blk.1.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:              blk.1.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.2.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:            blk.2.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:              blk.2.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:         blk.2.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:              blk.2.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:            blk.3.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:              blk.3.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:         blk.3.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:              blk.3.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.4.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:            blk.4.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:              blk.4.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:         blk.4.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:              blk.4.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.5.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:            blk.5.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:              blk.5.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:         blk.5.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:              blk.5.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.6.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:            blk.6.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:              blk.6.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:         blk.6.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:              blk.6.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.7.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:            blk.7.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:              blk.7.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:         blk.7.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:              blk.7.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:              blk.8.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:         blk.8.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:           blk.10.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:           blk.10.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:             blk.10.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:             blk.10.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:        blk.10.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:             blk.10.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:             blk.10.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:           blk.11.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:           blk.11.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:             blk.11.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:             blk.11.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:        blk.11.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.11.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.11.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.12.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:             blk.12.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:             blk.12.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:        blk.12.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.12.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:             blk.12.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:            blk.8.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:            blk.8.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:              blk.8.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:            blk.9.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:            blk.9.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:              blk.9.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:              blk.9.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:         blk.9.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:              blk.9.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:              blk.9.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.13.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:           blk.13.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:             blk.13.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:        blk.13.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:             blk.13.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.14.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:           blk.14.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:             blk.14.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:        blk.14.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:             blk.14.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.15.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:           blk.15.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.15.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:        blk.15.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:             blk.15.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.16.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:           blk.16.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:             blk.16.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:        blk.16.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:             blk.16.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:             blk.17.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:        blk.17.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:           blk.17.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:             blk.17.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.18.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:           blk.18.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:             blk.18.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:        blk.18.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:             blk.18.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.19.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:           blk.19.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:             blk.19.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:        blk.19.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:             blk.19.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.20.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:           blk.20.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:             blk.20.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:        blk.20.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:             blk.20.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:           blk.21.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:             blk.21.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:        blk.21.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:             blk.21.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.22.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:           blk.22.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:             blk.22.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:        blk.22.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:             blk.22.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.23.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:           blk.23.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:             blk.23.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:        blk.23.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:             blk.23.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.24.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:           blk.24.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:             blk.24.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:        blk.24.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:             blk.24.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.25.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:           blk.25.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:             blk.25.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:        blk.25.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:             blk.25.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.26.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:        blk.26.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:           blk.26.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:             blk.26.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.27.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:           blk.27.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:             blk.27.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:        blk.27.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:             blk.27.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.28.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:           blk.28.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:             blk.28.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:        blk.28.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:             blk.28.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.29.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:           blk.29.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:             blk.29.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:        blk.29.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:             blk.29.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:           blk.30.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:             blk.30.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:        blk.30.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:             blk.30.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:                    output.weight q8_0     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:           blk.30.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:           blk.31.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:           blk.31.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:             blk.31.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:        blk.31.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:             blk.31.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   4:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32     \n",
      "llama_model_loader: - kv  11:                          general.file_type u32     \n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32     \n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32     \n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32     \n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32     \n",
      "llama_model_loader: - kv  20:               general.quantization_version u32     \n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_print_meta: format           = unknown\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly Q8_0\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 7.17 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name   = huggingfaceh4_zephyr-7b-beta\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token = 2 '</s>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.09 MB\n",
      "llm_load_tensors: mem required  = 7338.73 MB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3096\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: kv self size  =  774.00 MB\n",
      "llama_new_context_with_model: compute buffer total size = 9.37 MB\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "\n",
    "# llama_model_path = \"../../models/zephyr-7b-beta.Q4_K_M.gguf\"\n",
    "llama_model_path = \"../../models/zephyr-7b-beta.Q8_0.gguf\"\n",
    "n_ctx=3096\n",
    "#Use Llama model for embedding\n",
    "embeddings = LlamaCppEmbeddings(model_path=llama_model_path, n_ctx=n_ctx) # , n_ctx=2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01d59c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  8337.68 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 10220.65 ms /    29 tokens (  352.44 ms per token,     2.84 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 10226.12 ms\n",
      "\n",
      "llama_print_timings:        load time =  8337.68 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 11633.12 ms /   126 tokens (   92.33 ms per token,    10.83 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 11653.17 ms\n",
      "\n",
      "llama_print_timings:        load time =  8337.68 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =   863.87 ms /     8 tokens (  107.98 ms per token,     9.26 tokens per second)\n",
      "llama_print_timings:        eval time =   502.49 ms /     1 runs   (  502.49 ms per token,     1.99 tokens per second)\n",
      "llama_print_timings:       total time =  1370.98 ms\n",
      "\n",
      "llama_print_timings:        load time =  8337.68 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  3819.41 ms /    64 tokens (   59.68 ms per token,    16.76 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  3829.03 ms\n",
      "\n",
      "llama_print_timings:        load time =  8337.68 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  8110.97 ms /   173 tokens (   46.88 ms per token,    21.33 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  8135.82 ms\n",
      "\n",
      "llama_print_timings:        load time =  8337.68 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =   856.00 ms /    21 tokens (   40.76 ms per token,    24.53 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   859.50 ms\n",
      "\n",
      "llama_print_timings:        load time =  8337.68 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  5296.35 ms /   128 tokens (   41.38 ms per token,    24.17 tokens per second)\n",
      "llama_print_timings:        eval time =    85.34 ms /     1 runs   (   85.34 ms per token,    11.72 tokens per second)\n",
      "llama_print_timings:       total time =  5399.16 ms\n",
      "\n",
      "llama_print_timings:        load time =  8337.68 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  7922.41 ms /   197 tokens (   40.22 ms per token,    24.87 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  7950.04 ms\n",
      "\n",
      "llama_print_timings:        load time =  8337.68 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  5473.99 ms /   132 tokens (   41.47 ms per token,    24.11 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  5491.78 ms\n",
      "\n",
      "llama_print_timings:        load time =  8337.68 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  2575.74 ms /    64 tokens (   40.25 ms per token,    24.85 tokens per second)\n",
      "llama_print_timings:        eval time =    83.00 ms /     1 runs   (   83.00 ms per token,    12.05 tokens per second)\n",
      "llama_print_timings:       total time =  2667.95 ms\n",
      "\n",
      "llama_print_timings:        load time =  8337.68 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  7982.39 ms /   200 tokens (   39.91 ms per token,    25.06 tokens per second)\n",
      "llama_print_timings:        eval time =    85.70 ms /     1 runs   (   85.70 ms per token,    11.67 tokens per second)\n",
      "llama_print_timings:       total time =  8095.22 ms\n",
      "\n",
      "llama_print_timings:        load time =  8337.68 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  5616.49 ms /   140 tokens (   40.12 ms per token,    24.93 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  5634.68 ms\n",
      "\n",
      "llama_print_timings:        load time =  8337.68 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  2979.23 ms /    68 tokens (   43.81 ms per token,    22.82 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  2988.89 ms\n",
      "\n",
      "llama_print_timings:        load time =  8337.68 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  5953.37 ms /   148 tokens (   40.23 ms per token,    24.86 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  5974.42 ms\n",
      "\n",
      "llama_print_timings:        load time =  8337.68 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  1693.67 ms /    42 tokens (   40.33 ms per token,    24.80 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  1699.84 ms\n",
      "\n",
      "llama_print_timings:        load time =  8337.68 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  3630.37 ms /    88 tokens (   41.25 ms per token,    24.24 tokens per second)\n",
      "llama_print_timings:        eval time =   371.65 ms /     1 runs   (  371.65 ms per token,     2.69 tokens per second)\n",
      "llama_print_timings:       total time =  4015.42 ms\n",
      "\n",
      "llama_print_timings:        load time =  8337.68 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  4342.56 ms /    78 tokens (   55.67 ms per token,    17.96 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  4355.68 ms\n",
      "\n",
      "llama_print_timings:        load time =  8337.68 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  2866.00 ms /    71 tokens (   40.37 ms per token,    24.77 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  2878.40 ms\n",
      "\n",
      "llama_print_timings:        load time =  8337.68 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  5121.86 ms /   122 tokens (   41.98 ms per token,    23.82 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  5139.17 ms\n",
      "\n",
      "llama_print_timings:        load time =  8337.68 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  4328.51 ms /   106 tokens (   40.84 ms per token,    24.49 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  4343.36 ms\n",
      "\n",
      "llama_print_timings:        load time =  8337.68 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  5072.63 ms /   127 tokens (   39.94 ms per token,    25.04 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  5090.92 ms\n",
      "\n",
      "llama_print_timings:        load time =  8337.68 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  4606.48 ms /   114 tokens (   40.41 ms per token,    24.75 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  4621.81 ms\n",
      "\n",
      "llama_print_timings:        load time =  8337.68 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  1058.77 ms /    26 tokens (   40.72 ms per token,    24.56 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  1062.30 ms\n",
      "\n",
      "llama_print_timings:        load time =  8337.68 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  4781.39 ms /   116 tokens (   41.22 ms per token,    24.26 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  4797.89 ms\n",
      "\n",
      "llama_print_timings:        load time =  8337.68 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  4810.46 ms /   118 tokens (   40.77 ms per token,    24.53 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  4826.61 ms\n",
      "\n",
      "llama_print_timings:        load time =  8337.68 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  4898.83 ms /   123 tokens (   39.83 ms per token,    25.11 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  4915.47 ms\n",
      "\n",
      "llama_print_timings:        load time =  8337.68 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =   104.57 ms /     2 tokens (   52.29 ms per token,    19.13 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =   105.43 ms\n",
      "\n",
      "llama_print_timings:        load time =  8337.68 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  4848.40 ms /   116 tokens (   41.80 ms per token,    23.93 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  4864.61 ms\n",
      "\n",
      "llama_print_timings:        load time =  8337.68 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  4508.77 ms /   110 tokens (   40.99 ms per token,    24.40 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  4523.86 ms\n",
      "\n",
      "llama_print_timings:        load time =  8337.68 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  4169.09 ms /   102 tokens (   40.87 ms per token,    24.47 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  4182.34 ms\n",
      "\n",
      "llama_print_timings:        load time =  8337.68 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  4679.50 ms /   106 tokens (   44.15 ms per token,    22.65 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  4695.19 ms\n",
      "\n",
      "llama_print_timings:        load time =  8337.68 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  3563.66 ms /    87 tokens (   40.96 ms per token,    24.41 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  3576.03 ms\n"
     ]
    }
   ],
   "source": [
    "# Build vectorstore and keep the metadata\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9525f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../../models/zephyr-7b-beta.Q8_0.gguf (version unknown)\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q8_0     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:            blk.0.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:            blk.0.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:              blk.0.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:         blk.0.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:              blk.0.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.1.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:            blk.1.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:              blk.1.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:         blk.1.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:              blk.1.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.2.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:            blk.2.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:              blk.2.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:         blk.2.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:              blk.2.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:            blk.3.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:              blk.3.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:         blk.3.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:              blk.3.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.4.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:            blk.4.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:              blk.4.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:         blk.4.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:              blk.4.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.5.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:            blk.5.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:              blk.5.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:         blk.5.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:              blk.5.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.6.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:            blk.6.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:              blk.6.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:         blk.6.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:              blk.6.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.7.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:            blk.7.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:              blk.7.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:         blk.7.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:              blk.7.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:              blk.8.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:         blk.8.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:           blk.10.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:           blk.10.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:             blk.10.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:             blk.10.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:        blk.10.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:             blk.10.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:             blk.10.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:           blk.11.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:           blk.11.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:             blk.11.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:             blk.11.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:        blk.11.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.11.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.11.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.12.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:             blk.12.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:             blk.12.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:        blk.12.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.12.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:             blk.12.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:            blk.8.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:            blk.8.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:              blk.8.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:            blk.9.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:            blk.9.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:              blk.9.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:              blk.9.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:         blk.9.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:              blk.9.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:              blk.9.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.13.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:           blk.13.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:             blk.13.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:        blk.13.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:             blk.13.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.14.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:           blk.14.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:             blk.14.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:        blk.14.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:             blk.14.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.15.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:           blk.15.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.15.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:        blk.15.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:             blk.15.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.16.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:           blk.16.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:             blk.16.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:        blk.16.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:             blk.16.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:             blk.17.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:        blk.17.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:           blk.17.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:             blk.17.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.18.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:           blk.18.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:             blk.18.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:        blk.18.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:             blk.18.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.19.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:           blk.19.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:             blk.19.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:        blk.19.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:             blk.19.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.20.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:           blk.20.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:             blk.20.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:        blk.20.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:             blk.20.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:           blk.21.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:             blk.21.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:        blk.21.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:             blk.21.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.22.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:           blk.22.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:             blk.22.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:        blk.22.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:             blk.22.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.23.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:           blk.23.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:             blk.23.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:        blk.23.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:             blk.23.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.24.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:           blk.24.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:             blk.24.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:        blk.24.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:             blk.24.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.25.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:           blk.25.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:             blk.25.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:        blk.25.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:             blk.25.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.26.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:        blk.26.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:           blk.26.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:             blk.26.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.27.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:           blk.27.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:             blk.27.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:        blk.27.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:             blk.27.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.28.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:           blk.28.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:             blk.28.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:        blk.28.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:             blk.28.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.29.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:           blk.29.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:             blk.29.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:        blk.29.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:             blk.29.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:           blk.30.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:             blk.30.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:        blk.30.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:             blk.30.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:                    output.weight q8_0     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:           blk.30.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:           blk.31.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:           blk.31.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:             blk.31.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:        blk.31.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:             blk.31.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   4:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32     \n",
      "llama_model_loader: - kv  11:                          general.file_type u32     \n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32     \n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32     \n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32     \n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32     \n",
      "llama_model_loader: - kv  20:               general.quantization_version u32     \n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_print_meta: format           = unknown\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly Q8_0\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 7.17 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name   = huggingfaceh4_zephyr-7b-beta\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token = 2 '</s>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.09 MB\n",
      "llm_load_tensors: mem required  = 7338.73 MB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3096\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: kv self size  =  387.00 MB\n",
      "llama_new_context_with_model: compute buffer total size = 229.43 MB\n",
      "llama_new_context_with_model: max tensor size =   132.81 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7339.34 MB, ( 7339.97 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   389.00 MB, ( 7728.97 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =   223.56 MB, ( 7952.53 / 10922.67)\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "temperature=0\n",
    "n_gpu_layers = 1  # Metal set to 1 is enough.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=llama_model_path,\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=n_ctx,\n",
    "    temperature=temperature,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310346dd",
   "metadata": {},
   "source": [
    "Let's create a `SelfQueryRetriever` that can filter based upon metadata we defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fd4d283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "\n",
    "# Define our metadata\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"Section\",\n",
    "        description=\"Part of the document that the text comes from\",\n",
    "        type=\"string or list[string]\",\n",
    "    ),\n",
    "]\n",
    "document_content_description = \"Major sections of the document\"\n",
    "\n",
    "# Define self query retriever\n",
    "# llm = OpenAI(temperature=0)\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm, vectorstore, document_content_description, metadata_field_info, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218b9820",
   "metadata": {},
   "source": [
    "We can see that we can query *only for texts* in the `Introduction` of the document!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d688db6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  2652.96 ms\n",
      "llama_print_timings:      sample time =   349.37 ms /   256 runs   (    1.36 ms per token,   732.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 14436.50 ms /   256 runs   (   56.39 ms per token,    17.73 tokens per second)\n",
      "llama_print_timings:       total time = 15410.67 ms\n"
     ]
    },
    {
     "ename": "OutputParserException",
     "evalue": "Parsing text\n```json\n{\n    \"query\": \"\",\n    \"filter\": \"eq(\\\"Section\\\", \\\"Introduction\\\")\"\n}\n```\n\n\n<< Example 4. >>\nData Source:\n```json\n{\n    \"content\": \"Major sections of the document\",\n    \"attributes\": {\n        \"Section\": {\n            \"description\": \"Part of the document that the text comes from\",\n            \"type\": \"string or list[string]\"\n        }\n    }\n}\n```\n\nUser Query:\nSummarize all sections of the document except for Introduction and Conclusion\n\nStructured Request:\n```json\n{\n    \"query\": \"\",\n    \"filter\": \"and(ne(\\\"Section\\\", \\\"Introduction\\\"), ne(\\\"Section\\\", \\\"Conclusion\\\"))\"\n}\n```\n\n\n<< Example 5. >>\nData Source:\n```json\n{\n    \"content\": \"Major sections of the document\",\n    \"attributes\": {\n        \"Section\": {\n            \"description\": \"Part of the document that the text comes from\",\n            \"type\": \"string or list\n raised following error:\nGot invalid JSON object. Error: Extra data: line 5 column 1 (char 71)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/output_parsers/json.py:163\u001b[0m, in \u001b[0;36mparse_and_check_json_markdown\u001b[0;34m(text, expected_keys)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     json_obj \u001b[39m=\u001b[39m parse_json_markdown(text)\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m json\u001b[39m.\u001b[39mJSONDecodeError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/output_parsers/json.py:145\u001b[0m, in \u001b[0;36mparse_json_markdown\u001b[0;34m(json_string, parser)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39m# Parse the JSON string into a Python dictionary\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m parsed \u001b[39m=\u001b[39m parser(json_str)\n\u001b[1;32m    147\u001b[0m \u001b[39mreturn\u001b[39;00m parsed\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/json/decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n\u001b[0;32m--> 340\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExtra data\u001b[39m\u001b[39m\"\u001b[39m, s, end)\n\u001b[1;32m    341\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 5 column 1 (char 71)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/chains/query_constructor/base.py:47\u001b[0m, in \u001b[0;36mStructuredQueryOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     46\u001b[0m allowed_keys \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfilter\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlimit\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> 47\u001b[0m parsed \u001b[39m=\u001b[39m parse_and_check_json_markdown(text, expected_keys)\n\u001b[1;32m     48\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(parsed[\u001b[39m\"\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/output_parsers/json.py:165\u001b[0m, in \u001b[0;36mparse_and_check_json_markdown\u001b[0;34m(text, expected_keys)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m json\u001b[39m.\u001b[39mJSONDecodeError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 165\u001b[0m     \u001b[39mraise\u001b[39;00m OutputParserException(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot invalid JSON object. Error: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    166\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m expected_keys:\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Got invalid JSON object. Error: Extra data: line 5 column 1 (char 71)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m/Users/furyhawk/projects/llama2/document-context-aware-QA.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/furyhawk/projects/llama2/document-context-aware-QA.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Test\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/furyhawk/projects/llama2/document-context-aware-QA.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m retriever\u001b[39m.\u001b[39;49mget_relevant_documents(\u001b[39m\"\u001b[39;49m\u001b[39mSummarize the Introduction section of the document\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/schema/retriever.py:211\u001b[0m, in \u001b[0;36mBaseRetriever.get_relevant_documents\u001b[0;34m(self, query, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    210\u001b[0m     run_manager\u001b[39m.\u001b[39mon_retriever_error(e)\n\u001b[0;32m--> 211\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    212\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     run_manager\u001b[39m.\u001b[39mon_retriever_end(\n\u001b[1;32m    214\u001b[0m         result,\n\u001b[1;32m    215\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/schema/retriever.py:204\u001b[0m, in \u001b[0;36mBaseRetriever.get_relevant_documents\u001b[0;34m(self, query, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m _kwargs \u001b[39m=\u001b[39m kwargs \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expects_other_args \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m    203\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new_arg_supported:\n\u001b[0;32m--> 204\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_relevant_documents(\n\u001b[1;32m    205\u001b[0m         query, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_kwargs\n\u001b[1;32m    206\u001b[0m     )\n\u001b[1;32m    207\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_relevant_documents(query, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_kwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/retrievers/self_query/base.py:160\u001b[0m, in \u001b[0;36mSelfQueryRetriever._get_relevant_documents\u001b[0;34m(self, query, run_manager)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_relevant_documents\u001b[39m(\n\u001b[1;32m    150\u001b[0m     \u001b[39mself\u001b[39m, query: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m, run_manager: CallbackManagerForRetrieverRun\n\u001b[1;32m    151\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[1;32m    152\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get documents relevant for a query.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \n\u001b[1;32m    154\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39m        List of relevant documents\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m     structured_query \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquery_constructor\u001b[39m.\u001b[39;49minvoke(\n\u001b[1;32m    161\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mquery\u001b[39;49m\u001b[39m\"\u001b[39;49m: query}, config\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mcallbacks\u001b[39;49m\u001b[39m\"\u001b[39;49m: run_manager\u001b[39m.\u001b[39;49mget_child()}\n\u001b[1;32m    162\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose:\n\u001b[1;32m    164\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGenerated Query: \u001b[39m\u001b[39m{\u001b[39;00mstructured_query\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/schema/runnable/base.py:1153\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1152\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[0;32m-> 1153\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[1;32m   1154\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[1;32m   1155\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   1156\u001b[0m             patch_config(\n\u001b[1;32m   1157\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   1158\u001b[0m             ),\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[1;32m   1160\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/schema/output_parser.py:183\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_config(\n\u001b[1;32m    175\u001b[0m         \u001b[39mlambda\u001b[39;00m inner_input: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_result(\n\u001b[1;32m    176\u001b[0m             [ChatGeneration(message\u001b[39m=\u001b[39minner_input)]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    180\u001b[0m         run_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparser\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    181\u001b[0m     )\n\u001b[1;32m    182\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 183\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_with_config(\n\u001b[1;32m    184\u001b[0m         \u001b[39mlambda\u001b[39;49;00m inner_input: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparse_result([Generation(text\u001b[39m=\u001b[39;49minner_input)]),\n\u001b[1;32m    185\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    186\u001b[0m         config,\n\u001b[1;32m    187\u001b[0m         run_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mparser\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    188\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/schema/runnable/base.py:668\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m    661\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    662\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    663\u001b[0m     \u001b[39minput\u001b[39m,\n\u001b[1;32m    664\u001b[0m     run_type\u001b[39m=\u001b[39mrun_type,\n\u001b[1;32m    665\u001b[0m     name\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_name\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    666\u001b[0m )\n\u001b[1;32m    667\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 668\u001b[0m     output \u001b[39m=\u001b[39m call_func_with_variable_args(\n\u001b[1;32m    669\u001b[0m         func, \u001b[39minput\u001b[39;49m, config, run_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    670\u001b[0m     )\n\u001b[1;32m    671\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    672\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/schema/runnable/config.py:259\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[39mif\u001b[39;00m run_manager \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    258\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m run_manager\n\u001b[0;32m--> 259\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/schema/output_parser.py:184\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[0;34m(inner_input)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_config(\n\u001b[1;32m    175\u001b[0m         \u001b[39mlambda\u001b[39;00m inner_input: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_result(\n\u001b[1;32m    176\u001b[0m             [ChatGeneration(message\u001b[39m=\u001b[39minner_input)]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    180\u001b[0m         run_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparser\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    181\u001b[0m     )\n\u001b[1;32m    182\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_config(\n\u001b[0;32m--> 184\u001b[0m         \u001b[39mlambda\u001b[39;00m inner_input: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparse_result([Generation(text\u001b[39m=\u001b[39;49minner_input)]),\n\u001b[1;32m    185\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[1;32m    186\u001b[0m         config,\n\u001b[1;32m    187\u001b[0m         run_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparser\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    188\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/schema/output_parser.py:226\u001b[0m, in \u001b[0;36mBaseOutputParser.parse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse_result\u001b[39m(\u001b[39mself\u001b[39m, result: List[Generation], \u001b[39m*\u001b[39m, partial: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    214\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Parse a list of candidate model Generations into a specific format.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \n\u001b[1;32m    216\u001b[0m \u001b[39m    The return value is parsed from only the first Generation in the result, which\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[39m        Structured output.\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparse(result[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mtext)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/chains/query_constructor/base.py:60\u001b[0m, in \u001b[0;36mStructuredQueryOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[39mreturn\u001b[39;00m StructuredQuery(\n\u001b[1;32m     57\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m parsed\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m allowed_keys}\n\u001b[1;32m     58\u001b[0m     )\n\u001b[1;32m     59\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m---> 60\u001b[0m     \u001b[39mraise\u001b[39;00m OutputParserException(\n\u001b[1;32m     61\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mParsing text\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m raised following error:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m     )\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Parsing text\n```json\n{\n    \"query\": \"\",\n    \"filter\": \"eq(\\\"Section\\\", \\\"Introduction\\\")\"\n}\n```\n\n\n<< Example 4. >>\nData Source:\n```json\n{\n    \"content\": \"Major sections of the document\",\n    \"attributes\": {\n        \"Section\": {\n            \"description\": \"Part of the document that the text comes from\",\n            \"type\": \"string or list[string]\"\n        }\n    }\n}\n```\n\nUser Query:\nSummarize all sections of the document except for Introduction and Conclusion\n\nStructured Request:\n```json\n{\n    \"query\": \"\",\n    \"filter\": \"and(ne(\\\"Section\\\", \\\"Introduction\\\"), ne(\\\"Section\\\", \\\"Conclusion\\\"))\"\n}\n```\n\n\n<< Example 5. >>\nData Source:\n```json\n{\n    \"content\": \"Major sections of the document\",\n    \"attributes\": {\n        \"Section\": {\n            \"description\": \"Part of the document that the text comes from\",\n            \"type\": \"string or list\n raised following error:\nGot invalid JSON object. Error: Extra data: line 5 column 1 (char 71)"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "retriever.get_relevant_documents(\"Summarize the Introduction section of the document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8064987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query='Introduction' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='Section', value='Introduction') limit=None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='![Untitled](Auto-Evaluation%20of%20Metadata%20Filtering%2018502448c85240828f33716740f9574b/Untitled.png)', metadata={'Section': 'Introduction'}),\n",
       " Document(page_content='Q+A systems often use a two-step approach: retrieve relevant text chunks and then synthesize them into an answer. There many ways to approach this. For example, we recently [discussed](https://blog.langchain.dev/auto-evaluation-of-anthropic-100k-context-window/) the Retriever-Less option (at bottom in the below diagram), highlighting the Anthropic 100k context window model. Metadata filtering is an alternative approach that pre-filters chunks based on a user-defined criteria in a VectorDB using', metadata={'Section': 'Introduction'}),\n",
       " Document(page_content='metadata tags prior to semantic search.', metadata={'Section': 'Introduction'})]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "retriever.get_relevant_documents(\"Summarize the Introduction section of the document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35999b3",
   "metadata": {},
   "source": [
    "We can also look at other parts of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47929be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  2652.96 ms\n",
      "llama_print_timings:      sample time =   353.51 ms /   256 runs   (    1.38 ms per token,   724.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =   626.89 ms /    13 tokens (   48.22 ms per token,    20.74 tokens per second)\n",
      "llama_print_timings:        eval time = 13828.42 ms /   255 runs   (   54.23 ms per token,    18.44 tokens per second)\n",
      "llama_print_timings:       total time = 15441.65 ms\n"
     ]
    },
    {
     "ename": "OutputParserException",
     "evalue": "Parsing text\n```json\n{\n    \"query\": \"Testing\",\n    \"filter\": \"eq(\\\"Section\\\", \\\"Testing\\\")\"\n}\n```\n\n\n<< Example 4. >>\nData Source:\n```json\n{\n    \"content\": \"Major sections of the document\",\n    \"attributes\": {\n        \"Section\": {\n            \"description\": \"Part of the document that the text comes from\",\n            \"type\": \"string or list[string]\"\n        }\n    }\n}\n```\n\nUser Query:\nSummarize all sections of the document except for the Introduction section\n\nStructured Request:\n```json\n{\n    \"query\": \"\",\n    \"filter\": \"and(ne(\\\"Section\\\", \\\"Introduction\\\"), NO_FILTER)\"\n}\n```\n\n\n<< Example 5. >>\nData Source:\n```json\n{\n    \"content\": \"Major sections of the document\",\n    \"attributes\": {\n        \"Section\": {\n            \"description\": \"Part of the document that the text comes from\",\n            \"type\": \"string or list[string]\"\n       \n raised following error:\nGot invalid JSON object. Error: Extra data: line 5 column 1 (char 73)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/output_parsers/json.py:163\u001b[0m, in \u001b[0;36mparse_and_check_json_markdown\u001b[0;34m(text, expected_keys)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     json_obj \u001b[39m=\u001b[39m parse_json_markdown(text)\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m json\u001b[39m.\u001b[39mJSONDecodeError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/output_parsers/json.py:145\u001b[0m, in \u001b[0;36mparse_json_markdown\u001b[0;34m(json_string, parser)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39m# Parse the JSON string into a Python dictionary\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m parsed \u001b[39m=\u001b[39m parser(json_str)\n\u001b[1;32m    147\u001b[0m \u001b[39mreturn\u001b[39;00m parsed\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/json/decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n\u001b[0;32m--> 340\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExtra data\u001b[39m\u001b[39m\"\u001b[39m, s, end)\n\u001b[1;32m    341\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 5 column 1 (char 73)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/chains/query_constructor/base.py:47\u001b[0m, in \u001b[0;36mStructuredQueryOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     46\u001b[0m allowed_keys \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfilter\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlimit\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> 47\u001b[0m parsed \u001b[39m=\u001b[39m parse_and_check_json_markdown(text, expected_keys)\n\u001b[1;32m     48\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(parsed[\u001b[39m\"\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/output_parsers/json.py:165\u001b[0m, in \u001b[0;36mparse_and_check_json_markdown\u001b[0;34m(text, expected_keys)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m json\u001b[39m.\u001b[39mJSONDecodeError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 165\u001b[0m     \u001b[39mraise\u001b[39;00m OutputParserException(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot invalid JSON object. Error: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    166\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m expected_keys:\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Got invalid JSON object. Error: Extra data: line 5 column 1 (char 73)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m/Users/furyhawk/projects/llama2/document-context-aware-QA.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/furyhawk/projects/llama2/document-context-aware-QA.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m retriever\u001b[39m.\u001b[39;49mget_relevant_documents(\u001b[39m\"\u001b[39;49m\u001b[39mSummarize the Testing section of the document\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/schema/retriever.py:211\u001b[0m, in \u001b[0;36mBaseRetriever.get_relevant_documents\u001b[0;34m(self, query, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    210\u001b[0m     run_manager\u001b[39m.\u001b[39mon_retriever_error(e)\n\u001b[0;32m--> 211\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    212\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     run_manager\u001b[39m.\u001b[39mon_retriever_end(\n\u001b[1;32m    214\u001b[0m         result,\n\u001b[1;32m    215\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/schema/retriever.py:204\u001b[0m, in \u001b[0;36mBaseRetriever.get_relevant_documents\u001b[0;34m(self, query, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m _kwargs \u001b[39m=\u001b[39m kwargs \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expects_other_args \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m    203\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new_arg_supported:\n\u001b[0;32m--> 204\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_relevant_documents(\n\u001b[1;32m    205\u001b[0m         query, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_kwargs\n\u001b[1;32m    206\u001b[0m     )\n\u001b[1;32m    207\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_relevant_documents(query, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_kwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/retrievers/self_query/base.py:160\u001b[0m, in \u001b[0;36mSelfQueryRetriever._get_relevant_documents\u001b[0;34m(self, query, run_manager)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_relevant_documents\u001b[39m(\n\u001b[1;32m    150\u001b[0m     \u001b[39mself\u001b[39m, query: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m, run_manager: CallbackManagerForRetrieverRun\n\u001b[1;32m    151\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[1;32m    152\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get documents relevant for a query.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \n\u001b[1;32m    154\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39m        List of relevant documents\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m     structured_query \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquery_constructor\u001b[39m.\u001b[39;49minvoke(\n\u001b[1;32m    161\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mquery\u001b[39;49m\u001b[39m\"\u001b[39;49m: query}, config\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mcallbacks\u001b[39;49m\u001b[39m\"\u001b[39;49m: run_manager\u001b[39m.\u001b[39;49mget_child()}\n\u001b[1;32m    162\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose:\n\u001b[1;32m    164\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGenerated Query: \u001b[39m\u001b[39m{\u001b[39;00mstructured_query\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/schema/runnable/base.py:1153\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1152\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[0;32m-> 1153\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[1;32m   1154\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[1;32m   1155\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   1156\u001b[0m             patch_config(\n\u001b[1;32m   1157\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   1158\u001b[0m             ),\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[1;32m   1160\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/schema/output_parser.py:183\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_config(\n\u001b[1;32m    175\u001b[0m         \u001b[39mlambda\u001b[39;00m inner_input: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_result(\n\u001b[1;32m    176\u001b[0m             [ChatGeneration(message\u001b[39m=\u001b[39minner_input)]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    180\u001b[0m         run_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparser\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    181\u001b[0m     )\n\u001b[1;32m    182\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 183\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_with_config(\n\u001b[1;32m    184\u001b[0m         \u001b[39mlambda\u001b[39;49;00m inner_input: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparse_result([Generation(text\u001b[39m=\u001b[39;49minner_input)]),\n\u001b[1;32m    185\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    186\u001b[0m         config,\n\u001b[1;32m    187\u001b[0m         run_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mparser\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    188\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/schema/runnable/base.py:668\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m    661\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    662\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    663\u001b[0m     \u001b[39minput\u001b[39m,\n\u001b[1;32m    664\u001b[0m     run_type\u001b[39m=\u001b[39mrun_type,\n\u001b[1;32m    665\u001b[0m     name\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_name\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    666\u001b[0m )\n\u001b[1;32m    667\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 668\u001b[0m     output \u001b[39m=\u001b[39m call_func_with_variable_args(\n\u001b[1;32m    669\u001b[0m         func, \u001b[39minput\u001b[39;49m, config, run_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    670\u001b[0m     )\n\u001b[1;32m    671\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    672\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/schema/runnable/config.py:259\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[39mif\u001b[39;00m run_manager \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    258\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m run_manager\n\u001b[0;32m--> 259\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/schema/output_parser.py:184\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[0;34m(inner_input)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_config(\n\u001b[1;32m    175\u001b[0m         \u001b[39mlambda\u001b[39;00m inner_input: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_result(\n\u001b[1;32m    176\u001b[0m             [ChatGeneration(message\u001b[39m=\u001b[39minner_input)]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    180\u001b[0m         run_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparser\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    181\u001b[0m     )\n\u001b[1;32m    182\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_config(\n\u001b[0;32m--> 184\u001b[0m         \u001b[39mlambda\u001b[39;00m inner_input: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparse_result([Generation(text\u001b[39m=\u001b[39;49minner_input)]),\n\u001b[1;32m    185\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[1;32m    186\u001b[0m         config,\n\u001b[1;32m    187\u001b[0m         run_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparser\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    188\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/schema/output_parser.py:226\u001b[0m, in \u001b[0;36mBaseOutputParser.parse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse_result\u001b[39m(\u001b[39mself\u001b[39m, result: List[Generation], \u001b[39m*\u001b[39m, partial: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    214\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Parse a list of candidate model Generations into a specific format.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \n\u001b[1;32m    216\u001b[0m \u001b[39m    The return value is parsed from only the first Generation in the result, which\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[39m        Structured output.\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparse(result[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mtext)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/chains/query_constructor/base.py:60\u001b[0m, in \u001b[0;36mStructuredQueryOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[39mreturn\u001b[39;00m StructuredQuery(\n\u001b[1;32m     57\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m parsed\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m allowed_keys}\n\u001b[1;32m     58\u001b[0m     )\n\u001b[1;32m     59\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m---> 60\u001b[0m     \u001b[39mraise\u001b[39;00m OutputParserException(\n\u001b[1;32m     61\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mParsing text\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m raised following error:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m     )\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Parsing text\n```json\n{\n    \"query\": \"Testing\",\n    \"filter\": \"eq(\\\"Section\\\", \\\"Testing\\\")\"\n}\n```\n\n\n<< Example 4. >>\nData Source:\n```json\n{\n    \"content\": \"Major sections of the document\",\n    \"attributes\": {\n        \"Section\": {\n            \"description\": \"Part of the document that the text comes from\",\n            \"type\": \"string or list[string]\"\n        }\n    }\n}\n```\n\nUser Query:\nSummarize all sections of the document except for the Introduction section\n\nStructured Request:\n```json\n{\n    \"query\": \"\",\n    \"filter\": \"and(ne(\\\"Section\\\", \\\"Introduction\\\"), NO_FILTER)\"\n}\n```\n\n\n<< Example 5. >>\nData Source:\n```json\n{\n    \"content\": \"Major sections of the document\",\n    \"attributes\": {\n        \"Section\": {\n            \"description\": \"Part of the document that the text comes from\",\n            \"type\": \"string or list[string]\"\n       \n raised following error:\nGot invalid JSON object. Error: Extra data: line 5 column 1 (char 73)"
     ]
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"Summarize the Testing section of the document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af7720f",
   "metadata": {},
   "source": [
    "Now, we can create chat or Q+A apps that are aware of the explicit document structure. \n",
    "\n",
    "The ability to retain document structure for metadata filtering can be helpful for complicated or longer documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "565822a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  2652.96 ms\n",
      "llama_print_timings:      sample time =   348.42 ms /   256 runs   (    1.36 ms per token,   734.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 14064.33 ms /   256 runs   (   54.94 ms per token,    18.20 tokens per second)\n",
      "llama_print_timings:       total time = 15046.22 ms\n"
     ]
    },
    {
     "ename": "OutputParserException",
     "evalue": "Parsing text\n```json\n{\n    \"query\": \"Testing\",\n    \"filter\": \"eq(\\\"Section\\\", \\\"Testing\\\")\"\n}\n```\n\n\n<< Example 4. >>\nData Source:\n```json\n{\n    \"content\": \"Major sections of the document\",\n    \"attributes\": {\n        \"Section\": {\n            \"description\": \"Part of the document that the text comes from\",\n            \"type\": \"string or list[string]\"\n        }\n    }\n}\n```\n\nUser Query:\nSummarize all sections of the document except for the Introduction section\n\nStructured Request:\n```json\n{\n    \"query\": \"\",\n    \"filter\": \"and(ne(\\\"Section\\\", \\\"Introduction\\\"), NO_FILTER)\"\n}\n```\n\n\n<< Example 5. >>\nData Source:\n```json\n{\n    \"content\": \"Major sections of the document\",\n    \"attributes\": {\n        \"Section\": {\n            \"description\": \"Part of the document that the text comes from\",\n            \"type\": \"string or list[string]\"\n       \n raised following error:\nGot invalid JSON object. Error: Extra data: line 5 column 1 (char 73)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/output_parsers/json.py:163\u001b[0m, in \u001b[0;36mparse_and_check_json_markdown\u001b[0;34m(text, expected_keys)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     json_obj \u001b[39m=\u001b[39m parse_json_markdown(text)\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m json\u001b[39m.\u001b[39mJSONDecodeError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/output_parsers/json.py:145\u001b[0m, in \u001b[0;36mparse_json_markdown\u001b[0;34m(json_string, parser)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39m# Parse the JSON string into a Python dictionary\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m parsed \u001b[39m=\u001b[39m parser(json_str)\n\u001b[1;32m    147\u001b[0m \u001b[39mreturn\u001b[39;00m parsed\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/json/decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n\u001b[0;32m--> 340\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExtra data\u001b[39m\u001b[39m\"\u001b[39m, s, end)\n\u001b[1;32m    341\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 5 column 1 (char 73)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/chains/query_constructor/base.py:47\u001b[0m, in \u001b[0;36mStructuredQueryOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     46\u001b[0m allowed_keys \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfilter\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlimit\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> 47\u001b[0m parsed \u001b[39m=\u001b[39m parse_and_check_json_markdown(text, expected_keys)\n\u001b[1;32m     48\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(parsed[\u001b[39m\"\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/output_parsers/json.py:165\u001b[0m, in \u001b[0;36mparse_and_check_json_markdown\u001b[0;34m(text, expected_keys)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m json\u001b[39m.\u001b[39mJSONDecodeError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 165\u001b[0m     \u001b[39mraise\u001b[39;00m OutputParserException(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot invalid JSON object. Error: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    166\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m expected_keys:\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Got invalid JSON object. Error: Extra data: line 5 column 1 (char 73)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m/Users/furyhawk/projects/llama2/document-context-aware-QA.ipynb Cell 19\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/furyhawk/projects/llama2/document-context-aware-QA.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# from langchain.chat_models import ChatOpenAI\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/furyhawk/projects/llama2/document-context-aware-QA.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/furyhawk/projects/llama2/document-context-aware-QA.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/furyhawk/projects/llama2/document-context-aware-QA.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m qa_chain \u001b[39m=\u001b[39m RetrievalQA\u001b[39m.\u001b[39mfrom_chain_type(llm, retriever\u001b[39m=\u001b[39mretriever)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/furyhawk/projects/llama2/document-context-aware-QA.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m qa_chain\u001b[39m.\u001b[39;49mrun(\u001b[39m\"\u001b[39;49m\u001b[39mSummarize the Testing section of the document\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/chains/base.py:505\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    504\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 505\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m], callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags, metadata\u001b[39m=\u001b[39;49mmetadata)[\n\u001b[1;32m    506\u001b[0m         _output_key\n\u001b[1;32m    507\u001b[0m     ]\n\u001b[1;32m    509\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    510\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags, metadata\u001b[39m=\u001b[39mmetadata)[\n\u001b[1;32m    511\u001b[0m         _output_key\n\u001b[1;32m    512\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/chains/base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    311\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    312\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    313\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    314\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/chains/base.py:304\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    297\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    298\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    299\u001b[0m     inputs,\n\u001b[1;32m    300\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[1;32m    301\u001b[0m )\n\u001b[1;32m    302\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 304\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    305\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    306\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    307\u001b[0m     )\n\u001b[1;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/chains/retrieval_qa/base.py:136\u001b[0m, in \u001b[0;36mBaseRetrievalQA._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    132\u001b[0m accepts_run_manager \u001b[39m=\u001b[39m (\n\u001b[1;32m    133\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_docs)\u001b[39m.\u001b[39mparameters\n\u001b[1;32m    134\u001b[0m )\n\u001b[1;32m    135\u001b[0m \u001b[39mif\u001b[39;00m accepts_run_manager:\n\u001b[0;32m--> 136\u001b[0m     docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_docs(question, run_manager\u001b[39m=\u001b[39;49m_run_manager)\n\u001b[1;32m    137\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_docs(question)  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/chains/retrieval_qa/base.py:216\u001b[0m, in \u001b[0;36mRetrievalQA._get_docs\u001b[0;34m(self, question, run_manager)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_docs\u001b[39m(\n\u001b[1;32m    210\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    211\u001b[0m     question: \u001b[39mstr\u001b[39m,\n\u001b[1;32m    212\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[1;32m    213\u001b[0m     run_manager: CallbackManagerForChainRun,\n\u001b[1;32m    214\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[1;32m    215\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get docs.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretriever\u001b[39m.\u001b[39;49mget_relevant_documents(\n\u001b[1;32m    217\u001b[0m         question, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child()\n\u001b[1;32m    218\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/schema/retriever.py:211\u001b[0m, in \u001b[0;36mBaseRetriever.get_relevant_documents\u001b[0;34m(self, query, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    210\u001b[0m     run_manager\u001b[39m.\u001b[39mon_retriever_error(e)\n\u001b[0;32m--> 211\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    212\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     run_manager\u001b[39m.\u001b[39mon_retriever_end(\n\u001b[1;32m    214\u001b[0m         result,\n\u001b[1;32m    215\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/schema/retriever.py:204\u001b[0m, in \u001b[0;36mBaseRetriever.get_relevant_documents\u001b[0;34m(self, query, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m _kwargs \u001b[39m=\u001b[39m kwargs \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expects_other_args \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m    203\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new_arg_supported:\n\u001b[0;32m--> 204\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_relevant_documents(\n\u001b[1;32m    205\u001b[0m         query, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_kwargs\n\u001b[1;32m    206\u001b[0m     )\n\u001b[1;32m    207\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_relevant_documents(query, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_kwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/retrievers/self_query/base.py:160\u001b[0m, in \u001b[0;36mSelfQueryRetriever._get_relevant_documents\u001b[0;34m(self, query, run_manager)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_relevant_documents\u001b[39m(\n\u001b[1;32m    150\u001b[0m     \u001b[39mself\u001b[39m, query: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m, run_manager: CallbackManagerForRetrieverRun\n\u001b[1;32m    151\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[1;32m    152\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get documents relevant for a query.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \n\u001b[1;32m    154\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39m        List of relevant documents\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m     structured_query \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquery_constructor\u001b[39m.\u001b[39;49minvoke(\n\u001b[1;32m    161\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mquery\u001b[39;49m\u001b[39m\"\u001b[39;49m: query}, config\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mcallbacks\u001b[39;49m\u001b[39m\"\u001b[39;49m: run_manager\u001b[39m.\u001b[39;49mget_child()}\n\u001b[1;32m    162\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose:\n\u001b[1;32m    164\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGenerated Query: \u001b[39m\u001b[39m{\u001b[39;00mstructured_query\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/schema/runnable/base.py:1153\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1152\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[0;32m-> 1153\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[1;32m   1154\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[1;32m   1155\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   1156\u001b[0m             patch_config(\n\u001b[1;32m   1157\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   1158\u001b[0m             ),\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[1;32m   1160\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/schema/output_parser.py:183\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_config(\n\u001b[1;32m    175\u001b[0m         \u001b[39mlambda\u001b[39;00m inner_input: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_result(\n\u001b[1;32m    176\u001b[0m             [ChatGeneration(message\u001b[39m=\u001b[39minner_input)]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    180\u001b[0m         run_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparser\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    181\u001b[0m     )\n\u001b[1;32m    182\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 183\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_with_config(\n\u001b[1;32m    184\u001b[0m         \u001b[39mlambda\u001b[39;49;00m inner_input: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparse_result([Generation(text\u001b[39m=\u001b[39;49minner_input)]),\n\u001b[1;32m    185\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    186\u001b[0m         config,\n\u001b[1;32m    187\u001b[0m         run_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mparser\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    188\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/schema/runnable/base.py:668\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m    661\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    662\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    663\u001b[0m     \u001b[39minput\u001b[39m,\n\u001b[1;32m    664\u001b[0m     run_type\u001b[39m=\u001b[39mrun_type,\n\u001b[1;32m    665\u001b[0m     name\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_name\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    666\u001b[0m )\n\u001b[1;32m    667\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 668\u001b[0m     output \u001b[39m=\u001b[39m call_func_with_variable_args(\n\u001b[1;32m    669\u001b[0m         func, \u001b[39minput\u001b[39;49m, config, run_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    670\u001b[0m     )\n\u001b[1;32m    671\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    672\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/schema/runnable/config.py:259\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[39mif\u001b[39;00m run_manager \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    258\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m run_manager\n\u001b[0;32m--> 259\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/schema/output_parser.py:184\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[0;34m(inner_input)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_config(\n\u001b[1;32m    175\u001b[0m         \u001b[39mlambda\u001b[39;00m inner_input: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_result(\n\u001b[1;32m    176\u001b[0m             [ChatGeneration(message\u001b[39m=\u001b[39minner_input)]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    180\u001b[0m         run_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparser\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    181\u001b[0m     )\n\u001b[1;32m    182\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_config(\n\u001b[0;32m--> 184\u001b[0m         \u001b[39mlambda\u001b[39;00m inner_input: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparse_result([Generation(text\u001b[39m=\u001b[39;49minner_input)]),\n\u001b[1;32m    185\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[1;32m    186\u001b[0m         config,\n\u001b[1;32m    187\u001b[0m         run_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparser\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    188\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/schema/output_parser.py:226\u001b[0m, in \u001b[0;36mBaseOutputParser.parse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse_result\u001b[39m(\u001b[39mself\u001b[39m, result: List[Generation], \u001b[39m*\u001b[39m, partial: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    214\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Parse a list of candidate model Generations into a specific format.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \n\u001b[1;32m    216\u001b[0m \u001b[39m    The return value is parsed from only the first Generation in the result, which\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[39m        Structured output.\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparse(result[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mtext)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.11/site-packages/langchain/chains/query_constructor/base.py:60\u001b[0m, in \u001b[0;36mStructuredQueryOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[39mreturn\u001b[39;00m StructuredQuery(\n\u001b[1;32m     57\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m parsed\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m allowed_keys}\n\u001b[1;32m     58\u001b[0m     )\n\u001b[1;32m     59\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m---> 60\u001b[0m     \u001b[39mraise\u001b[39;00m OutputParserException(\n\u001b[1;32m     61\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mParsing text\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m raised following error:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m     )\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Parsing text\n```json\n{\n    \"query\": \"Testing\",\n    \"filter\": \"eq(\\\"Section\\\", \\\"Testing\\\")\"\n}\n```\n\n\n<< Example 4. >>\nData Source:\n```json\n{\n    \"content\": \"Major sections of the document\",\n    \"attributes\": {\n        \"Section\": {\n            \"description\": \"Part of the document that the text comes from\",\n            \"type\": \"string or list[string]\"\n        }\n    }\n}\n```\n\nUser Query:\nSummarize all sections of the document except for the Introduction section\n\nStructured Request:\n```json\n{\n    \"query\": \"\",\n    \"filter\": \"and(ne(\\\"Section\\\", \\\"Introduction\\\"), NO_FILTER)\"\n}\n```\n\n\n<< Example 5. >>\nData Source:\n```json\n{\n    \"content\": \"Major sections of the document\",\n    \"attributes\": {\n        \"Section\": {\n            \"description\": \"Part of the document that the text comes from\",\n            \"type\": \"string or list[string]\"\n       \n raised following error:\nGot invalid JSON object. Error: Extra data: line 5 column 1 (char 73)"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever)\n",
    "qa_chain.run(\"Summarize the Testing section of the document\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
