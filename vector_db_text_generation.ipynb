{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve from vector stores directly\n",
    "\n",
    "This notebook walks through how to use LangChain for text generation over a vector index. This is useful if we want to generate text that is able to draw from a large body of custom text, for example, generating blog posts that have an understanding of previous blog posts written, or product tutorials that can refer to product documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "First, we prepare the data. For this example, we fetch a documentation site that consists of markdown files hosted on Github and split them into small enough Documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.llms import OpenAI\n",
    "from langchain.docstore.document import Document\n",
    "import requests\n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "import pathlib\n",
    "import subprocess\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into '.'...\n"
     ]
    }
   ],
   "source": [
    "def get_github_docs(repo_owner, repo_name):\n",
    "    with tempfile.TemporaryDirectory() as d:\n",
    "        subprocess.check_call(\n",
    "            f\"git clone --depth 1 https://github.com/{repo_owner}/{repo_name}.git .\",\n",
    "            cwd=d,\n",
    "            shell=True,\n",
    "        )\n",
    "        git_sha = (\n",
    "            subprocess.check_output(\"git rev-parse HEAD\", shell=True, cwd=d)\n",
    "            .decode(\"utf-8\")\n",
    "            .strip()\n",
    "        )\n",
    "        repo_path = pathlib.Path(d)\n",
    "        markdown_files = list(repo_path.glob(\"*/*.md\")) + list(\n",
    "            repo_path.glob(\"*/*.mdx\")\n",
    "        )\n",
    "        for markdown_file in markdown_files:\n",
    "            with open(markdown_file, \"r\") as f:\n",
    "                relative_path = markdown_file.relative_to(repo_path)\n",
    "                github_url = f\"https://github.com/{repo_owner}/{repo_name}/blob/{git_sha}/{relative_path}\"\n",
    "                yield Document(page_content=f.read(), metadata={\"source\": github_url})\n",
    "\n",
    "\n",
    "sources = get_github_docs(\"yirenlu92\", \"deno-manual-forked\")\n",
    "\n",
    "source_chunks = []\n",
    "splitter = CharacterTextSplitter(separator=\" \", chunk_size=1024, chunk_overlap=0)\n",
    "for source in sources:\n",
    "    for chunk in splitter.split_text(source.page_content):\n",
    "        source_chunks.append(Document(page_content=chunk, metadata=source.metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Vector DB\n",
    "\n",
    "Now that we have the documentation content in chunks, let's put all this information in a vector index for easy retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../../models/zephyr-7b-beta.Q8_0.gguf (version unknown)\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q8_0     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:            blk.0.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:            blk.0.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:              blk.0.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:         blk.0.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:              blk.0.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.1.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:            blk.1.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:              blk.1.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:         blk.1.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:              blk.1.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.2.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:            blk.2.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:              blk.2.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:         blk.2.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:              blk.2.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:            blk.3.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:              blk.3.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:         blk.3.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:              blk.3.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.4.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:            blk.4.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:              blk.4.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:         blk.4.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:              blk.4.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.5.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:            blk.5.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:              blk.5.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:         blk.5.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:              blk.5.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.6.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:            blk.6.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:              blk.6.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:         blk.6.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:              blk.6.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.7.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:            blk.7.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:              blk.7.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:         blk.7.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:              blk.7.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:              blk.8.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:         blk.8.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:           blk.10.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:           blk.10.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:             blk.10.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:             blk.10.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:        blk.10.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:             blk.10.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:             blk.10.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:           blk.11.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:           blk.11.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:             blk.11.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:             blk.11.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:        blk.11.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.11.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.11.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.12.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:             blk.12.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:             blk.12.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:        blk.12.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.12.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:             blk.12.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:            blk.8.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:            blk.8.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:              blk.8.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:            blk.9.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:            blk.9.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:              blk.9.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:              blk.9.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:         blk.9.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:              blk.9.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:              blk.9.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.13.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:           blk.13.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:             blk.13.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:        blk.13.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:             blk.13.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.14.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:           blk.14.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:             blk.14.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:        blk.14.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:             blk.14.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.15.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:           blk.15.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.15.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:        blk.15.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:             blk.15.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.16.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:           blk.16.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:             blk.16.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:        blk.16.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:             blk.16.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:             blk.17.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:        blk.17.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:           blk.17.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:             blk.17.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.18.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:           blk.18.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:             blk.18.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:        blk.18.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:             blk.18.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.19.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:           blk.19.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:             blk.19.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:        blk.19.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:             blk.19.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.20.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:           blk.20.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:             blk.20.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:        blk.20.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:             blk.20.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:           blk.21.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:             blk.21.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:        blk.21.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:             blk.21.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.22.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:           blk.22.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:             blk.22.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:        blk.22.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:             blk.22.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.23.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:           blk.23.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:             blk.23.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:        blk.23.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:             blk.23.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.24.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:           blk.24.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:             blk.24.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:        blk.24.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:             blk.24.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.25.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:           blk.25.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:             blk.25.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:        blk.25.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:             blk.25.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.26.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:        blk.26.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:           blk.26.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:             blk.26.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.27.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:           blk.27.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:             blk.27.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:        blk.27.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:             blk.27.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.28.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:           blk.28.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:             blk.28.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:        blk.28.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:             blk.28.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.29.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:           blk.29.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:             blk.29.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:        blk.29.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:             blk.29.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:           blk.30.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:             blk.30.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:        blk.30.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:             blk.30.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:                    output.weight q8_0     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:           blk.30.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:           blk.31.ffn_down.weight q8_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:           blk.31.ffn_gate.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.ffn_up.weight q8_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:             blk.31.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:        blk.31.attn_output.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.attn_q.weight q8_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:             blk.31.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   4:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32     \n",
      "llama_model_loader: - kv  11:                          general.file_type u32     \n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32     \n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32     \n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32     \n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32     \n",
      "llama_model_loader: - kv  20:               general.quantization_version u32     \n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_print_meta: format           = unknown\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly Q8_0\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 7.17 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name   = huggingfaceh4_zephyr-7b-beta\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token = 2 '</s>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.09 MB\n",
      "llm_load_tensors: mem required  = 7338.73 MB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3096\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: kv self size  =  774.00 MB\n",
      "llama_new_context_with_model: compute buffer total size = 9.37 MB\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "\n",
    "# llama_model_path = \"../../models/zephyr-7b-beta.Q4_K_M.gguf\"\n",
    "llama_model_path = \"../../models/zephyr-7b-beta.Q8_0.gguf\"\n",
    "n_ctx=3096\n",
    "#Use Llama model for embedding\n",
    "embeddings = LlamaCppEmbeddings(model_path=llama_model_path, n_ctx=n_ctx) # , n_ctx=2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 462008.85 ms /   331 tokens ( 1395.80 ms per token,     0.72 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 462080.37 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 165080.33 ms /   114 tokens ( 1448.07 ms per token,     0.69 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 165110.22 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 390460.56 ms /   250 tokens ( 1561.84 ms per token,     0.64 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 390518.46 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 1019325.84 ms /   260 tokens ( 3920.48 ms per token,     0.26 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 1019395.06 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 353304.16 ms /   256 tokens ( 1380.09 ms per token,     0.72 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 353362.60 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 1497615.14 ms /   356 tokens ( 4206.78 ms per token,     0.24 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 1497694.37 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 591770.36 ms /   351 tokens ( 1685.96 ms per token,     0.59 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 591932.21 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 445229.22 ms /   272 tokens ( 1636.87 ms per token,     0.61 tokens per second)\n",
      "llama_print_timings:        eval time = 12150.93 ms /     1 runs   (12150.93 ms per token,     0.08 tokens per second)\n",
      "llama_print_timings:       total time = 457484.10 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 104813.12 ms /    62 tokens ( 1690.53 ms per token,     0.59 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 104837.97 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 441691.32 ms /   270 tokens ( 1635.89 ms per token,     0.61 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 441811.02 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 1436419.95 ms /   349 tokens ( 4115.82 ms per token,     0.24 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 1436526.57 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 461491.17 ms /   286 tokens ( 1613.61 ms per token,     0.62 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 461553.57 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 420118.81 ms /   274 tokens ( 1533.28 ms per token,     0.65 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 420184.54 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 437673.80 ms /   312 tokens ( 1402.80 ms per token,     0.71 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 437741.56 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 486704.96 ms /   344 tokens ( 1414.84 ms per token,     0.71 tokens per second)\n",
      "llama_print_timings:        eval time = 10989.59 ms /     1 runs   (10989.59 ms per token,     0.09 tokens per second)\n",
      "llama_print_timings:       total time = 497772.18 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 419548.46 ms /   296 tokens ( 1417.39 ms per token,     0.71 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 419611.82 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 389180.46 ms /   270 tokens ( 1441.41 ms per token,     0.69 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 389239.54 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 57787.01 ms /    36 tokens ( 1605.19 ms per token,     0.62 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 57798.90 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 495100.83 ms /   331 tokens ( 1495.77 ms per token,     0.67 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 495171.82 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 511197.24 ms /   351 tokens ( 1456.40 ms per token,     0.69 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 511275.59 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 505936.82 ms /   344 tokens ( 1470.75 ms per token,     0.68 tokens per second)\n",
      "llama_print_timings:        eval time = 11672.89 ms /     1 runs   (11672.89 ms per token,     0.09 tokens per second)\n",
      "llama_print_timings:       total time = 517689.71 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 461501.26 ms /   306 tokens ( 1508.17 ms per token,     0.66 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 461573.38 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 550095.87 ms /   357 tokens ( 1540.88 ms per token,     0.65 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 550174.14 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 543979.00 ms /   347 tokens ( 1567.66 ms per token,     0.64 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 544056.68 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 454474.75 ms /   299 tokens ( 1519.98 ms per token,     0.66 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 454542.68 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 451257.08 ms /   301 tokens ( 1499.19 ms per token,     0.67 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 451321.70 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 479816.03 ms /   323 tokens ( 1485.50 ms per token,     0.67 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 479890.09 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 446290.54 ms /   302 tokens ( 1477.78 ms per token,     0.68 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 446355.32 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 486845.32 ms /   327 tokens ( 1488.82 ms per token,     0.67 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 486917.76 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 454821.48 ms /   311 tokens ( 1462.45 ms per token,     0.68 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 454891.14 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 266547.43 ms /   192 tokens ( 1388.27 ms per token,     0.72 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 266591.98 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 492487.14 ms /   351 tokens ( 1403.10 ms per token,     0.71 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 492564.10 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 502281.78 ms /   359 tokens ( 1399.11 ms per token,     0.71 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 502363.84 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 505723.71 ms /   357 tokens ( 1416.59 ms per token,     0.71 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 505799.87 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 538917.50 ms /   380 tokens ( 1418.20 ms per token,     0.71 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 538999.47 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 481272.42 ms /   343 tokens ( 1403.13 ms per token,     0.71 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 481347.36 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 480116.73 ms /   344 tokens ( 1395.69 ms per token,     0.72 tokens per second)\n",
      "llama_print_timings:        eval time = 10837.21 ms /     1 runs   (10837.21 ms per token,     0.09 tokens per second)\n",
      "llama_print_timings:       total time = 491032.89 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 438894.22 ms /   312 tokens ( 1406.71 ms per token,     0.71 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 438963.13 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 394491.32 ms /   276 tokens ( 1429.32 ms per token,     0.70 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 394552.94 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 147950.92 ms /   103 tokens ( 1436.42 ms per token,     0.70 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 147975.83 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 414492.96 ms /   292 tokens ( 1419.50 ms per token,     0.70 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 414557.81 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 516927.59 ms /   363 tokens ( 1424.04 ms per token,     0.70 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 517008.63 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 401192.21 ms /   279 tokens ( 1437.96 ms per token,     0.70 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 401253.56 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 463600.52 ms /   327 tokens ( 1417.74 ms per token,     0.71 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 463674.47 ms\n",
      "\n",
      "llama_print_timings:        load time = 10848.14 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 122065.82 ms /    84 tokens ( 1453.16 ms per token,     0.69 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 122085.16 ms\n"
     ]
    }
   ],
   "source": [
    "search_index = Chroma.from_documents(source_chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up LLM Chain with Custom Prompt\n",
    "\n",
    "Next, let's set up a simple LLM chain but give it a custom prompt for blog post generation. Note that the custom prompt is parameterized and takes two inputs: `context`, which will be the documents fetched from the vector search, and `topic`, which is given by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "temperature=0\n",
    "n_gpu_layers = 1  # Metal set to 1 is enough.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=llama_model_path,\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=n_ctx,\n",
    "    temperature=temperature,\n",
    "    grammer_path=\"json.gbnf\",\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt_template = \"\"\"Use the context below to write a 400 word blog post about the topic below:\n",
    "    Context: {context}\n",
    "    Topic: {topic}\n",
    "    Blog post:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"topic\"])\n",
    "\n",
    "# llm = OpenAI(temperature=0)\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text\n",
    "\n",
    "Finally, we write a function to apply our inputs to the chain. The function takes an input parameter `topic`. We find the documents in the vector index that correspond to that `topic`, and use them as additional context in our simple LLM chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_blog_post(topic):\n",
    "    docs = search_index.similarity_search(topic, k=4)\n",
    "    inputs = [{\"context\": doc.page_content, \"topic\": topic} for doc in docs]\n",
    "    print(chain.apply(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': '\\n\\nEnvironment variables are a great way to store and access sensitive information in your Deno applications. Deno offers built-in support for environment variables with `Deno.env`, and you can also use a `.env` file to store and access environment variables.\\n\\nUsing `Deno.env` is simple. It has getter and setter methods, so you can easily set and retrieve environment variables. For example, you can set the `FIREBASE_API_KEY` and `FIREBASE_AUTH_DOMAIN` environment variables like this:\\n\\n```ts\\nDeno.env.set(\"FIREBASE_API_KEY\", \"examplekey123\");\\nDeno.env.set(\"FIREBASE_AUTH_DOMAIN\", \"firebasedomain.com\");\\n\\nconsole.log(Deno.env.get(\"FIREBASE_API_KEY\")); // examplekey123\\nconsole.log(Deno.env.get(\"FIREBASE_AUTH_DOMAIN\")); // firebasedomain.com\\n```\\n\\nYou can also store environment variables in a `.env` file. This is a great'}, {'text': '\\n\\nEnvironment variables are a powerful tool for managing configuration settings in a program. They allow us to set values that can be used by the program, without having to hard-code them into the code. This makes it easier to change settings without having to modify the code.\\n\\nIn Deno, environment variables can be set in a few different ways. The most common way is to use the `VAR=value` syntax. This will set the environment variable `VAR` to the value `value`. This can be used to set any number of environment variables before running a command. For example, if we wanted to set the environment variable `VAR` to `hello` before running a Deno command, we could do so like this:\\n\\n```\\nVAR=hello deno run main.ts\\n```\\n\\nThis will set the environment variable `VAR` to `hello` before running the command. We can then access this variable in our code using the `Deno.env.get()` function. For example, if we ran the following command:\\n\\n```\\nVAR=hello && deno eval \"console.log(\\'Deno: \\' + Deno.env.get(\\'VAR'}, {'text': '\\n\\nEnvironment variables are a powerful tool for developers, allowing them to store and access data without having to hard-code it into their applications. In Deno, you can access environment variables using the `Deno.env.get()` function.\\n\\nFor example, if you wanted to access the `HOME` environment variable, you could do so like this:\\n\\n```js\\n// env.js\\nDeno.env.get(\"HOME\");\\n```\\n\\nWhen running this code, you\\'ll need to grant the Deno process access to environment variables. This can be done by passing the `--allow-env` flag to the `deno run` command. You can also specify which environment variables you want to grant access to, like this:\\n\\n```shell\\n# Allow access to only the HOME env var\\ndeno run --allow-env=HOME env.js\\n```\\n\\nIt\\'s important to note that environment variables are case insensitive on Windows, so Deno also matches them case insensitively (on Windows only).\\n\\nAnother thing to be aware of when using environment variables is subprocess permissions. Subprocesses are powerful and can access system resources regardless of the permissions you granted to the Den'}, {'text': '\\n\\nEnvironment variables are an important part of any programming language, and Deno is no exception. Deno is a secure JavaScript and TypeScript runtime built on the V8 JavaScript engine, and it recently added support for environment variables. This feature was added in Deno version 1.6.0, and it is now available for use in Deno applications.\\n\\nEnvironment variables are used to store information that can be used by programs. They are typically used to store configuration information, such as the location of a database or the name of a user. In Deno, environment variables are stored in the `Deno.env` object. This object is similar to the `process.env` object in Node.js, and it allows you to access and set environment variables.\\n\\nThe `Deno.env` object is a read-only object, meaning that you cannot directly modify the environment variables. Instead, you must use the `Deno.env.set()` function to set environment variables. This function takes two arguments: the name of the environment variable and the value to set it to. For example, if you wanted to set the `FOO` environment variable to `bar`, you would use the following code:\\n\\n```'}]\n"
     ]
    }
   ],
   "source": [
    "generate_blog_post(\"environment variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
